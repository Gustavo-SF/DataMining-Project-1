{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospecção de Dados\n",
    "\n",
    "## Sara C. Madeira and André Falcão, 2019/20\n",
    "\n",
    "# Project 1 - Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "For this project we decided to test both an Apriori and an FPGrowth algorithm in the first exercise for the purpose of Pattern Mining. In accordance with the first Project for Data Mining course in FCUL, we analysed a dataset named **`Foodmart_2020_PD.csv`**.\n",
    "\n",
    "This dataset stores **69549 transactions** from **24 stores**, where **103 different products** can be bought. Each transaction (row) has a STORE_ID (integer from 1 to 24), and a list of products (items), together with the quantities bought. In the transation highlighted below, a customer bought 1 unit of soup, 2 of cheese and 1 of wine at store 2.\n",
    "\n",
    "<img src=\"Foodmart_2020_PD_Example.png\" alt=\"Foodmart_2020_PD_Example\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    "\n",
    "Imports are done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import fpmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mining Frequent Itemsets and Association Rules: Ignoring Product Quantities and Stores\n",
    "\n",
    "In this first part of the project you should load and preprocessed the dataset **`Foodmart_2020_PD.csv`** in order to compute frequent itemsets and generate association rules considering all the transactions, regardeless of the store, and ignoring product quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and Preprocess Dataset\n",
    "\n",
    " **Product quantities and stores should not be considered.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itemset = pd.read_csv('./data/Foodmart_2020_PD.csv', delimiter = ';')\n",
    "df_itemset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of 69549 transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe have 11 columns, which means that the transaction with most products has 11 products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_itemset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, preprocessing will be done in order to remove the store column and the quantities from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_quantities(x):\n",
    "    if isinstance(x,str):\n",
    "        item = x.split('=')\n",
    "        return item[0]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def remove_nan(transactions):\n",
    "    #remove nan values\n",
    "    transactions_cleaned = []\n",
    "    for i, t in enumerate(transactions):\n",
    "        transaction = []\n",
    "        for v in t: \n",
    "            if isinstance(v,str): transaction.append(v)\n",
    "        transactions_cleaned.append(transaction)\n",
    "    return transactions_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove store column from dataset and clear quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove store column\n",
    "df1 = df_itemset.loc[:,'Column2':'Column11']\n",
    "\n",
    "#clear quantities\n",
    "df1 = df1.applymap(clear_quantities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing quantities and store column, we get the DataFrame above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets convert the Dataframe to a matrix and remove the 'NaN' values in each vector (transaction), before fed the matrix into the Transaction Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_cleaned = remove_nan(df1.values)\n",
    "\n",
    "# View just the first 5 transactions\n",
    "transactions_cleaned[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the Transaction Encoder, it transforms our data into the input format of FP-Growth and Apriori algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions_cleaned).transform(transactions_cleaned)\n",
    "df_te = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df_te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction encoder dataframe has the following structure:\n",
    "\n",
    "* Each row represents one transaction;\n",
    "* Each column represents a product, and it is boolean: assumes True if the transaction contains that product, and False otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe outputed by the transaction has 102 columns, which means that our dataset has 102 unique products in all transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Small exploratory data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the products that we have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_frequency = pd.DataFrame()\n",
    "for c in df_te.columns:\n",
    "    df_c = df_te[c].value_counts().to_frame('count')\n",
    "    total = int(df_c[df_c.index == True]['count']) # Count True values\n",
    "    df_products_frequency = df_products_frequency.append({'product':c,'total': total,'support':(total/len(df_itemset))},ignore_index = True)\n",
    "df_products_frequency = df_products_frequency.sort_values('total',ascending = False).reset_index(drop=True)\n",
    "\n",
    "# Making it more presentable\n",
    "df_products_frequency['support'] = df_products_frequency['support'].astype('float64').round(3)\n",
    "df_products_frequency['total'] = df_products_frequency['total'].astype('int64')\n",
    "\n",
    "df_products_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most bought products are Fresh Vegatable, Fresh Fruit, Soup, Cheese and Dried Fruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fresh Vegatable has a support of 0.284, which means that 28% of the transactions (19764 transactions) include this product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some statistic metrics about the total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_frequency.total.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that on average each product appears in 2834 transactions (+/- 2589.67 std). The less purchased product appears in 895 transactions, and the most bought product appears in 19764 transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the most bought product is Fresh Vegatable, now lets check the less purchased product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_frequency.iloc[df_products_frequency['total'].values.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The less purchased product is Fresh Fish, which appears in just 895 transactions, just 1.3% of the transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the 20 most frequent bought products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "df_graph = df_products_frequency.iloc[:20]\n",
    "plt.bar(df_graph['product'],df_graph['total'])\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows that Fresh Vegetables is by far the most purchased product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute frequent itemsets considering a minimum support of X%. \n",
    "* Present frequent itemsets organized by length (number of items). \n",
    "* List frequent 1-itemsets, 2-itemsets, 3-itemsets, etc with support of at least Y%.\n",
    "* Change X and Y when it makes sense and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use Apriori and FP Growth to compute frequent itemsets, compare their performances, and use that itemsets to compute association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.005\n",
    "Y = 0.007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Using apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit apriori(df_te, min_support=X, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The apriori algorithm takes +- 10 seconds per loop with min_support = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the frequent itemsets of length 1, 2 and 3. We will create a column named 'num_transactions' which indicates approximately the number of transactions were that item set occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets  = apriori(df_te, min_support=X, use_colnames=True)\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support',ascending = False)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets['num_transactions'] = [round(s*len(df_itemset)) for s in frequent_itemsets['support']]\n",
    "frequent_itemsets = frequent_itemsets.sort_values('support',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.query(f'length == 1 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.query(f'length == 2 & support > {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fresh Vegetables and Fresh Fruit are bought together in 3541 transactions, which represents 5.1% of the total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a minimum support of 0.7%, we get 159 itemsets of size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.query(f'length == 3 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.query(f'length == 4 & support > {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a support > 0.007 there is no frequent itemset with 4 products, and just one itemset with 3 products (Fresh Vegetables, Soup and Fresh Fruit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Using FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [1], the apriori algorithm suffers from two main performance issues:\n",
    "\n",
    "* It may still need to generate a huge number of candidate sets. For example, if there are\n",
    "104 frequent 1-itemsets, the Apriori algorithm will need to generate more than 107\n",
    "candidate 2-itemsets.\n",
    "\n",
    "* It may need to repeatedly scan the whole database and check a large set of candidates by\n",
    "pattern matching. It is costly to go over each transaction in the database to determine\n",
    "the support of the candidate itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent pattern growth, or simply FP-Growth, is a method that mines the complete set of frequent itemsets without such a costly candidate generation process [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute the frequent itemsets by using FP-Growth algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fpgrowth(df_te, min_support=X, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg  = fpgrowth(df_te, min_support=X, use_colnames=True)\n",
    "frequent_itemsets_fpg = frequent_itemsets_fpg.sort_values('support',ascending = False)\n",
    "frequent_itemsets_fpg['length'] = frequent_itemsets_fpg['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets_fpg['num_transactions'] = [round(s*len(df_itemset)) for s in frequent_itemsets_fpg['support']]\n",
    "frequent_itemsets_fpg = frequent_itemsets_fpg.sort_values('support',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg.query(f'length == 1 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg.query(f'length == 2 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg.query(f'length == 3 & support > {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that FP Growth is using a highly compact FP-tree, divide-and-conquer method, it is much faster than Apriori. We can observe this by the 10x increase in speed. Both results are the same. During the next exercises we will use just FP Growth.\n",
    "\n",
    "About the results, they translate the frequency of a certain set of items being bought, and the higher the length of the set, the lower generally is the support. We can see this by having only one example for a length of 3 and more than 100 examples for length of 1 and 2. With a minimum support of 0.007 we have 102 frequent itemsets with one product, 159 frequent itemsets with two products and just one frequent itemset of size 3. If Y > 0.007045 there will be no itemsets of size 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets increase Y and check them most frequent itemsets of size 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg.query(f'length == 1 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_fpg.query(f'length == 2 & support > {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increase Y we are minimizing our tolerance, and so, getting just the frequent itemsets that are more frequent according to the Y threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Y = 0.05, we get 67 frequent itemsets of size 1 and 5 of size 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fresh Vegetables and Fresh Fruit were bought together 3541 times, and Fresh Vegetables and Cookies were bought together 1928 times. It is interesting to note that in all frequents itemsets of length 2 that we got, Fresh Vegetables is always one of the items. This happens because Fresh Vegetables are the most frequent product in the dataset.\n",
    "\n",
    "An interesting thing to  note is that if we have a very low Y, we end up with more itemsets of length 2 than others, due to the large possible combinations we can have, but these tend to have a lower support so the higher the support, the higher the percentage of smaller length itemsets, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generate association rules with a minimum confidence of C%. \n",
    "* Generate association rules with a minimum lift L. \n",
    "* Generate association rules with both confidence >= C% and lift >= L.\n",
    "* Change C and L when it makes sense and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by define a confidence level of 30% and 1.1 of Lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.3 # Confidence\n",
    "L = 1.1 # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0)\n",
    "rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fi(rules, length, C, L):\n",
    "    return rules[ (rules['antecedent_len'] >= length) &\n",
    "         (rules['confidence'] >= C) &\n",
    "         (rules['lift'] > L)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_1 = generate_fi(rules, 1, C, L)\n",
    "association_rules_1 = association_rules_1.sort_values('confidence',ascending = False)\n",
    "association_rules_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider that anything with a confidence below 0.5 is 'not so confident' but a lift over 1 makes it likely to happen. Lift is a correlation metric.\n",
    "\n",
    "> When lift is less than 1, then the occurrence of A is negatively correlated with the occurrence of B, meaning that the occurrence of one likely leads to the absence of the other one. If the resulting value is greater than 1, then A and B are positively correlated, meaning that the occurrence of one implies the occurrence of the other. If the resulting value is equal to 1, then A and B are independent and there is no\n",
    "correlation between them [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the top 3 rules found for this frequent itemsets are the following:\n",
    "\n",
    "+ If someone buys an Hamburguer, they buy Fresh Vegetables with a confidence of +- 0.32;\n",
    "+ If someone buys Aspirin, they buy Fresh Vegetables with a confidence of 0.32;\n",
    "+ If someone buys Soup and Fresh Fruit, they buy Fresh Vegetables. This is the association rule with the highest confidence: 0.33 and lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all of that top 3 rules found, lift is higher than 1, meaning that the products are positive related between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets decrease the confidence to 0.2 and increase the lift to 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.2 # Confidence\n",
    "L = 1.2 # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_2 = generate_fi(rules, 1, C, L)\n",
    "association_rules_2 = association_rules_2.sort_values('confidence',ascending = False)\n",
    "association_rules_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decrease  C and increase Lift, the two association rules that we got are:\n",
    "\n",
    "* Who buys Rice also buis Fresh Fruit, with a confidence of 21% and a lift of 1.22;\n",
    "* Who buys TV Dinner also buis Fresh Fruit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Take a Look at Maximal Patterns: Compute Maximal Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An itemset X is a maximal frequent itemset (or max-itemset) in a data set D if X is frequent, and there exists no super-itemset Y such that X C Y and Y is frequent in D[1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the maximal frequent itemsets we will use fpmax algorithm from mlxtend. Then maximal frequent itemsets will be filtered by itemset size (1,2 or 3) according to Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.005\n",
    "Y = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent_itemsets with support >= 0.005\n",
    "#maximal_itemsets = frequent_itemsets\n",
    "#for row in frequent_itemsets.index:\n",
    "#    for comparison_row in frequent_itemsets.index:\n",
    "#        if row == comparison_row:\n",
    "#            continue\n",
    "#        if frequent_itemsets['itemsets'][row].issubset(frequent_itemsets['itemsets'][comparison_row]):\n",
    "#            maximal_itemsets = maximal_itemsets.drop([row], axis=0)\n",
    "#            break\n",
    "#maximal_itemsets = maximal_itemsets.sort_values('support',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_frequent_itemsets = fpmax(df_te, min_support=X, use_colnames=True)\n",
    "maximal_frequent_itemsets['length'] = maximal_frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "maximal_frequent_itemsets['num_transactions'] = [round(s*len(df_itemset)) for s in maximal_frequent_itemsets['support']]\n",
    "maximal_frequent_itemsets = maximal_frequent_itemsets.sort_values('support',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_frequent_itemsets['length'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a minimum support of 0.005 we get 25 maximal itemsets of size 1, 261 of size 2 and 3 itemsets of size 3. Lets check those itemsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_frequent_itemsets[(maximal_frequent_itemsets['length']==1) & (maximal_frequent_itemsets['support']>Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ibuprofen is a maximal itemset of size 1, that was bougth 1014 times, meaning that there is no superset that contains Ibuprofen and is frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_frequent_itemsets[(maximal_frequent_itemsets['length'] == 2) & (maximal_frequent_itemsets['support']>Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_frequent_itemsets[(maximal_frequent_itemsets['length'] == 3) & (maximal_frequent_itemsets['support']>Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal Frequent Itemsets are sets of items that do not have any superset that is also frequent. Maximal Patterns show us well supported sets which happen without any other frequent superset. Therefore we can say:\n",
    "\n",
    ">Fresh Vegetable and Dried Fruit are a frequent set, and there is no superset that contains these items and is frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to net that Ibuprofen is the most frequent maximal itemset of size 3, which appers in 1,4% of the transactions. This means that there is no superset (at least with a minimum support of X) that includes Ibuprofen that is frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Conclusions from Mining Frequent Patterns in All Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The transactions we have, as we can note, are from supermarkets or grocery stores; We will not be able to see sets of specific items from each store since the support is represented by: `(number of transactions where A is presents)/(all transactions)`. Bigger stores will dominate the dataset in this way.\n",
    "\n",
    "> A better approach would be do do an analysis for specific stores, as we will do after.\n",
    "\n",
    "> It is interesting to note that buying Fresh Fruit and Fresh Vegetables together represents 5% of all transactions, making it fair to say that it is a common thing to happen when people go shopping in comparison with the multitude of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mining Frequent Itemsets and Association Rules: Looking for Differences between Stores\n",
    "\n",
    "The 24 stores, whose transactions were analysed in Task 1, are in fact **different types of stores**:\n",
    "* Deluxe Supermarkets: STORE_ID = 8, 12, 13, 17, 19, 21\n",
    "* Gourmet Supermarkets: STORE_ID = 4, 6\n",
    "* Mid-Size Grocerys: STORE_ID = 9, 18, 20, 23\n",
    "* Small Grocerys: STORE_ID = 2, 5, 14, 22\n",
    "* Supermarkets: STORE_ID = 1, 3, 7, 10, 11, 15, 16\n",
    "\n",
    "In this context, in this second task you should compute frequent itemsets and association rules for specific groups of stores, and then compare the store specific results with those obtained when all transactions were analysed independently of the type of store. \n",
    "\n",
    "**The goal is to find similarities and differences in buying patterns according to the types of store. Do popular products change? Are there buying patterns specific to the type of store?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Analyse Deluxe Supermarkets and Gourmet Supermarkets\n",
    "\n",
    "Here you should analyse transactions from **Deluxe Supermarkets (STORE_ID = 8, 12, 13, 17, 19, 21) and Gourmet Supermarkets (STORE_ID = 4, 6) together**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the Deluxe Supermarkets and Gourmet Supermarkets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns = df_itemset.columns) # new Itemset\n",
    "for i in [8,12,13,17,19,21,4,6]:    \n",
    "    df2 = pd.concat([df2,df_itemset[df_itemset['Column1'] == f'STORE_ID={i}']])\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 31251 transactions in data from deluxe and gourmet supermarkets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing Data**\n",
    "\n",
    "Clear quantities and store column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove store column\n",
    "df2 = df2.loc[:,'Column2':'Column11']\n",
    "#clear quantities\n",
    "df2 = df2.applymap(clear_quantities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 'NaN' values from the transaction matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_cleaned2 = remove_nan(df2.values)\n",
    "\n",
    "#view just the first 5 transactions\n",
    "transactions_cleaned2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transactions_cleaned2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply transaction encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder().fit(transactions_cleaned2)\n",
    "te_ary2 = te.transform(transactions_cleaned2)\n",
    "df21_te = pd.DataFrame(te_ary2, columns=te.columns_)\n",
    "df21_te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataset only with the transactions from the Deluxe and Gourmet Supermarkets, and it is ready to be given as input to the Apriori algorithm, but before that we can analyse the most frequent products present in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the most frequent products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21_products_frequency = pd.DataFrame()\n",
    "for c in df21_te.columns:\n",
    "    df_c = df21_te[c].value_counts().to_frame('count')\n",
    "    total = int(df_c[df_c.index == True]['count']) # Count True values\n",
    "    df21_products_frequency = df21_products_frequency.append({'product':c,'total': total,'support':(total/len(df2))},ignore_index = True)\n",
    "df21_products_frequency = df21_products_frequency.sort_values('total',ascending = False).reset_index(drop=True)\n",
    "\n",
    "# Making it more presentable\n",
    "df21_products_frequency['support'] = df21_products_frequency['support'].astype('float64').round(3)\n",
    "df21_products_frequency['total'] = df21_products_frequency['total'].astype('int64')\n",
    "\n",
    "df21_products_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much difference in terms of support. Both these stores at least in most frequent items seem to be very representative of the whole. This might happen since they also represent half of the whole dataset.\n",
    "\n",
    "For this case Fresh Vegetables is  present in 29% of the transactions in these stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some statistic metrics about the total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21_products_frequency.total.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of overall statistical difference. We have again 102 different products, and all the values seem to be around half of what happens in the whole dataset. Again reaffirming that this dataset is representative of the whole.\n",
    "\n",
    "We can again see the top 20 most frequent bought products and they mostly match the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "df_graph = df21_products_frequency.iloc[:20]\n",
    "plt.bar(df_graph['product'],df_graph['total'])\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.005\n",
    "Y = 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets21_fpg  = fpgrowth(df21_te, min_support=X, use_colnames=True)\n",
    "frequent_itemsets21_fpg = frequent_itemsets21_fpg.sort_values('support',ascending = False)\n",
    "frequent_itemsets21_fpg['length'] = frequent_itemsets21_fpg['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets21_fpg['num_transactions'] = [round(s*len(df2)) for s in frequent_itemsets21_fpg['support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets21_fpg.query(f'length == 1 & support > {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets21_fpg.query(f'length == 2 & support > {Y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are trying just one example, nothing much changes again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.3 # Confidence\n",
    "L = 1.1 # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules21 = association_rules(frequent_itemsets21_fpg, metric=\"confidence\", min_threshold=0)\n",
    "rules21['antecedent_len'] = rules21['antecedents'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules3 = generate_fi(rules21, 1, C, L)\n",
    "association_rules3 = association_rules3.sort_values('support',ascending = False)\n",
    "association_rules3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.35 # Confidence\n",
    "L = 1.2  # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules4 = generate_fi(rules21, 1, C, L)\n",
    "association_rules4 = association_rules4.sort_values('support',ascending = False)\n",
    "association_rules4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.  Deluxe/Gourmet Supermarkets versus All Stores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is some difference in the results from the association rules but this is most possibly caused by slight variations in the confidence and lift values which filter more cases in the first task.\n",
    "\n",
    "> Overall the difference is not substantial and as has been said before, this dataset is very representative of the whole dataset in Task 1, which also happens due to the fact that these stores have half of all of the transactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Analyse Small Groceries [For groups of 2 and 3]\n",
    "\n",
    "Here you should analyse **Small Groceries (STORE_ID = 2, 5, 14, 22)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1.  Load/Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df23 = pd.DataFrame(columns = df_itemset.columns) # new Itemset\n",
    "for i in [2,5,14,22]:    \n",
    "    df23 = pd.concat([df23,df_itemset[df_itemset['Column1'] == f'STORE_ID={i}']])\n",
    "df23.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will be dealing with a lot less transactions (2278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove store column\n",
    "df23 = df23.loc[:,'Column2':'Column11']\n",
    "#clear quantities\n",
    "df23 = df23.applymap(clear_quantities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_cleaned3 = remove_nan(df23.values)\n",
    "\n",
    "#view just the first 5 transactions\n",
    "transactions_cleaned3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder().fit(transactions_cleaned3)\n",
    "te_ary3 = te.transform(transactions_cleaned3)\n",
    "df23_te = pd.DataFrame(te_ary3, columns=te.columns_)\n",
    "df23_te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is now ready to be introduced in the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Compute Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.005\n",
    "Y = 0.007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets23_fpg  = fpgrowth(df23_te, min_support=X, use_colnames=True)\n",
    "frequent_itemsets23_fpg = frequent_itemsets23_fpg.sort_values('support',ascending = False)\n",
    "frequent_itemsets23_fpg['length'] = frequent_itemsets23_fpg['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets23_fpg['num_transactions'] = [round(s*len(transactions_cleaned3)) for s in frequent_itemsets23_fpg['support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets23_fpg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets23_fpg[(frequent_itemsets23_fpg['length']==1) & (frequent_itemsets23_fpg['support']> Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets23_fpg[(frequent_itemsets23_fpg['length']==2) & (frequent_itemsets23_fpg['support']> Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets23_fpg[(frequent_itemsets23_fpg['length']==3) & (frequent_itemsets23_fpg['support']> Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are dealing with a smaller dataset which might deviate a lot more from the whole due to random chance. To have a support higher than 0.007 for example it is only needed to have around 17 transactions in these stores. The most bought products are still present and seen in this store with very similar values of support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.3 # Confidence\n",
    "L = 1.1 # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules23 = association_rules(frequent_itemsets23_fpg, metric=\"confidence\", min_threshold=0)\n",
    "rules23['antecedent_len'] = rules23['antecedents'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_4 = generate_fi(rules23, 1, C, L)\n",
    "association_rules_4 = association_rules_4.sort_values('support',ascending = False)\n",
    "association_rules_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets increase both the confidence and lift parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.4 # Confidence\n",
    "L = 1.3 # Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules_5 = generate_fi(rules23, 1, C, L)\n",
    "association_rules_5 = association_rules_5.sort_values('support',ascending = False)\n",
    "association_rules_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, there is very high values of lift and confidence in this dataset, allowing us to identify products that have a high correlation and confidence for happening. \n",
    "\n",
    "For example we can know from this that it seems highly likely that buying Paper Wipes and cholocate Candy will mean that the person will buy Fresh Vegetables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Small Groceries versus All Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The results are not so different from the general again, but as said before, they are subjected to a random chance with smaller transactions, therefore to be more confident of the outcomes, it is suggested to have higher values of support for considering a product frequent.\n",
    "\n",
    "> An interesting case was the attained association rules which seem to be the most affected by the low number of transactions and since Fresh Vegetables are something that is usually bought, correlation and confidence values get to be higher in several cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.  Deluxe/Gourmet Supermarkets versus Small Groceries  [For groups of 2 and 3]\n",
    "\n",
    "Discuss the similarities and diferences between the results obtained in task 2.1. (frequent itemsets and association rules found in transactions only from Deluxe/Gourmet Supermarkets) and those obtained in task 2.2. (frequent itemsets and association rules found in transactions only Small Groceries).\n",
    "\n",
    "> In the first case we have dealt with a much bigger dataset, which had half of the whole transactions, while the second, only 3%. This means that the first one will of course be much more similar to the whole dataset. Since in task 2.1 we had many more transactions, it also allowed us to have more confident values for Condidence and Lift, as these won't be so subjected to random chance.\n",
    "\n",
    "> Still, in both Tasks we had very similar frequent itemsets with high support, showing us how patterns can be identified even in smaller examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Jiawei Han, Micheline Kamber, and Jian Pei. 2011. Data Mining: Concepts and Techniques (3rd. ed.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
